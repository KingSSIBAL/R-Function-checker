---
title: "Performance Benchmarking"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Performance Benchmarking}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Overview

The autograder includes performance benchmarking features to compare your 
implementation's speed against the instructor's reference solution. This helps 
you learn to write efficient code.

## Integrated Benchmarking

The easiest way to benchmark your code is using the `benchmark` parameter 
in the main `autograder()` function:

```{r}
library(autograder)

# Run autograder with benchmarking enabled
result <- autograder("fibonacci", benchmark = TRUE)
```

This will:
1. Run all tests first
2. If all tests pass, run performance benchmarks
3. Display timing comparison with the reference solution

### Benchmark Output

```
=== Performance Benchmark ===
Running 50 benchmark iterations...

Your time:       0.000123 seconds (median)
Reference time:  0.000145 seconds (median)
Performance:     0.85x (FASTER)
```

### Benchmark Parameters

```{r}
autograder("fibonacci", 
           benchmark = TRUE,        # Enable benchmarking
           benchmark_runs = 100,    # More runs = more accurate
           benchmark_bonus = 10)    # Up to 10% bonus for fast code
```

### Performance Bonus

If your instructor enables performance bonuses, you can earn extra points 
for implementations faster than the reference:

```{r}
result <- autograder("fibonacci", 
                     benchmark = TRUE, 
                     benchmark_bonus = 10)

# If faster, you'll see:
# ☆ PERFORMANCE BONUS: +1.5 points (15% of max 10%)
```

The bonus formula:
- **2x faster** (ratio = 0.5) → 100% of max bonus
- **10% faster** (ratio = 0.9) → ~10% of max bonus
- **Not faster** (ratio ≥ 1.0) → No bonus

## Standalone Performance Comparison

For detailed performance analysis, you can also use the internal 
`compare_performance()` function:

```{r}
# Access internal function
perf <- autograder:::compare_performance("fibonacci", n_runs = 100)
```

## Understanding Results

The function returns a performance comparison object with these components:

```{r}
perf$student_median   # Median execution time for your function
perf$instructor_median # Median execution time for reference
perf$student_mean     # Mean execution time for your function
perf$instructor_mean  # Mean execution time for reference
perf$ratio            # How many times slower/faster you are
perf$n_runs           # Number of benchmark iterations
```

### Interpreting the Ratio

- **ratio < 1**: Your code is faster than the reference
- **ratio = 1**: Same performance
- **ratio > 1**: Your code is slower than the reference

Example output:
```
Performance Comparison for 'fibonacci'
======================================
                    Student    Instructor
Median time:        0.015ms    0.012ms
Mean time:          0.018ms    0.014ms
Std deviation:      0.005ms    0.003ms

Performance ratio: 1.25x (student/instructor)
Your implementation is 25% slower than the reference.

Based on 100 runs per implementation.
```

## Visualizing Results

Use the plot method for a visual comparison:

```{r}
plot(perf)
```

This creates a side-by-side boxplot showing the distribution of execution times 
for both implementations.

## Optimizing Your Code

### Common Performance Issues

#### 1. Growing Vectors

Bad (slow):
```{r}
# Growing a vector in a loop is O(n²)
result <- c()
for (i in 1:n) {
  result <- c(result, compute(i))
}
```

Good (fast):
```{r}
# Pre-allocate the vector - O(n)
result <- numeric(n)
for (i in 1:n) {
  result[i] <- compute(i)
}
```

#### 2. Redundant Computation

Bad (slow):
```{r}
# Calculating mean inside loop
for (i in 1:n) {
  deviation[i] <- x[i] - mean(x)  # mean(x) calculated n times!
}
```

Good (fast):
```{r}
# Calculate once, use many times
m <- mean(x)
for (i in 1:n) {
  deviation[i] <- x[i] - m
}
```

#### 3. Using Loops Instead of Vectorization

Bad (slow):
```{r}
# Loop-based sum
total <- 0
for (i in 1:length(x)) {
  total <- total + x[i]
}
```

Good (fast):
```{r}
# Vectorized
total <- sum(x)
```

#### 4. Inefficient Recursion

Bad (slow):
```{r}
# Naive recursive Fibonacci - O(2^n)
fib <- function(n) {
  if (n <= 2) return(1)
  fib(n-1) + fib(n-2)
}
```

Good (fast):
```{r}
# Iterative Fibonacci - O(n)
fib <- function(n) {
  if (n <= 0) return(integer(0))
  if (n == 1) return(1L)
  
  result <- integer(n)
  result[1:2] <- 1L
  for (i in 3:n) {
    result[i] <- result[i-1] + result[i-2]
  }
  result
}
```

### R-Specific Optimizations

#### Use Built-in Functions

```{r}
# Instead of manual implementation
x_sorted <- x[order(x)]

# Use built-in sort (implemented in C)
x_sorted <- sort(x)
```

#### Leverage Vectorization

```{r}
# Instead of loop
squares <- numeric(length(x))
for (i in seq_along(x)) {
  squares[i] <- x[i]^2
}

# Use vectorization
squares <- x^2
```

#### Apply Functions

```{r}
# Instead of loop over rows
for (i in 1:nrow(mat)) {
  row_sums[i] <- sum(mat[i, ])
}

# Use apply
row_sums <- apply(mat, 1, sum)

# Even better: use rowSums
row_sums <- rowSums(mat)
```

## Benchmarking Tips

### 1. Use Sufficient Runs

More runs give more reliable results:

```{r}
# Quick check
compare_performance("fibonacci", n_runs = 10)

# Reliable benchmark
compare_performance("fibonacci", n_runs = 1000)
```

### 2. Benchmark on Representative Input

The autograder uses the last test case by default. For comprehensive 
benchmarking, consider the input sizes your function will typically handle.

### 3. Watch for Variability

High standard deviation indicates inconsistent performance:
```
Std deviation:  0.500ms  (high variability)
```

This might indicate:
- Memory allocation overhead
- Garbage collection interference
- System background processes

### 4. Multiple Benchmarks

Run the benchmark several times to ensure consistency:

```{r}
for (i in 1:3) {
  print(compare_performance("fibonacci", n_runs = 100))
}
```

## When Performance Matters

Performance optimization is important when:

1. **Processing large datasets** - O(n²) vs O(n) makes a huge difference
2. **Called frequently** - Functions in tight loops should be fast
3. **Real-time applications** - Latency-sensitive code
4. **Competition/grading** - Some assignments grade on efficiency

Performance optimization is less important when:

1. **Correctness is at stake** - Always prefer correct over fast
2. **One-time operations** - Setup code that runs once
3. **Small inputs** - Optimization overhead may exceed savings
4. **Code readability** - Maintainable code is often better than micro-optimized code

## Example Workflow

```{r}
library(autograder)

# 1. Check available problems
list_problems()

# 2. Preview test cases
preview_tests("fibonacci")

# 3. Create your function
student_fibonacci <- function(n) {
  if (n <= 0) return(integer(0))
  if (n == 1) return(1L)
  
  fib <- integer(n)
  fib[1:2] <- 1L
  for (i in 3:n) {
    fib[i] <- fib[i-1] + fib[i-2]
  }
  fib
}

# 4. Test for correctness
result <- autograder("fibonacci")
print(result)

# 5. If all tests pass, benchmark performance
if (result$pass_rate == 100) {
  perf <- compare_performance("fibonacci", n_runs = 100)
  print(perf)
  plot(perf)
}
```
